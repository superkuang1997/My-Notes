# 从集中式到分布式🐽

## 集中式的特点

集中式系统由一台或多台计算机组成中心节点，数据集中存储于这个中心节点中，并且整个系统的所有业务单元都集中部署在这个中心节点上。

- 部署结构简单，无需考虑多个节点的分布式协作问题

- 存在明显的单点问题，虽然稳定性较好，但一旦出现故障，整个系统将处于不可用状态



## 分布式的特点

分布式系统是一个硬件和软件组件分布在不同网络的计算机上，彼此之间仅通过消息传递进行通信和协调的系统。

同一个分布式系统中的计算机在空间部署上是任意的，可以在不同的机房、城市中。

- 分布式系统中没有主从之分，所有的节点都是对等的
- 缺乏全局时钟，在分布式系统中很难定义事件发生的时间先后



## 分布式存在的问题

- 通信异常

  分布式节点之间需要通过网络进行通信，由于网络本身的不可靠性，每次通信都会存在网络不可用的风险。即使网络通信能够正常进行，其延时也会远远大于单机操作。

- 网络分区

  由于网络发生异常，会导致分布式系统中的部分节点之间网络延时不断增大，最终导致系统中只有部分节点能够正常通信，而另一些节点不能，这种现象叫做「网络分区」。网络分区出现时，分布式系统中会出现局部小集群，极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能。

- 三态

  传统单机系统中，程序调用函数后，可以得到明确的回应，即成功或失败；但在分布式系统中，由于网络是不可靠的，除了成功或失败，还可能出现超时现象。

  - 客户端在发往服务端的途中消息丢失
  - 消息被服务端成功接收并进行了处理，但向客户端回包时消息丢失

- 节点故障

  分布式系统中的每个节点每时每刻都可能会发生故障



# CAP🐽

## 一致性

一致性（Consistency）：多个数据副本是否能保持一致的特性，在一致性的条件下，系统在执行数据更新操作之后所有节点在同一时间的数据完全一致。

对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。



## 可用性

可用性（Availabiltity）：分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量。

在可用性条件下，要求系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。



## 分区容错性

网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。

分区容错性（Partition tolerance）：分布式系统中在遇到任何网络分区故障的时候，仍然需要保证对外提供满足一致性或可用性的服务，除非是整个网络环境都发生了故障。

一般来说，系统如果不能在时限内达成数据的最终一致性，就意味着发生了分区的情况。



## 三进二特性

分布式系统不可能同时满足一致性、可用性和分区容忍性，最多只能同时满足其中两项。

- CA：单点集群，满足一致性，可用性的系统，放弃了分区容错性往往意味着放弃了系统的可扩展性 

  > RDBMS

- CP：满足一致性，分区容忍性的系统，放弃了可用性意味着一旦系统遇到了网络分区或其他故障，那么受影响的服务需要等待一定时间去恢复，系统在这个时间段内不可用。

  > Redis、MongoDB、Hbase

- AP：满足可用性，分区容忍性的系统，这里放弃了一致性并不是完全放弃一致性，因为如果系统中的数据完全没有一致性，那么这些数据都是毫无意义的。所以放弃一致性指的是放弃数据的强一致性，保留数据的最终一致性。

  > 大多数网站架构的选择

<img src="http://store.secretcamp.cn/uPic/image-20210316211118187202103162111181615900278VT3GeKVT3GeK.png" alt="image-20210316211118187" style="zoom:33%;" />





在分布式系统中，分区容忍性（P）必不可少，这是一个基本要求，因为分布式系统中必然存在多个节点，同时网络又是不稳定的，所以保证分区容错性就是保证分布式系统正常运行的必然要求。

因此，CAP 理论实际上是要在可用性和一致性之间做权衡。可用性和一致性往往是冲突的，很难使它们同时满足。在多个节点之间进行数据同步时：

- 为了保证一致性（CP），不能访问未同步完成的节点，也就失去了部分可用性
- 为了保证可用性（AP），允许读取所有节点的数据，但是数据可能不一致



# BASE🐽

BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。

BASE 理论是对 CAP 中一致性和可用性权衡的结果，它的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。

BASE 理论的思想是通过让系统放松对某一时刻数据一致性的要求，来换取系統整体伸缩性和性能上改观。

许多大型系统往往由于地域分布和极高性能的要求，不可能采用分布式事务来完成某些指标，要想获得这些指标，我们必须采用另外一种方式来完成，这里 BASE 就是解决这个问题的办法。



## 基本可用

指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。

- 响应时间上的损失：在线搜索引擎原本可以在 0.5 秒返回查询结果，由于出现故障，响应时间增加到1~2秒
- 功能上的损失：电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面



## 软状态

指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在时延。



## 最终一致性

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。

ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。

在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。





# 分布式锁🐽

## 分布式锁的概念

在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。

高效分布式锁的必要条件：

- 互斥

  在分布式高并发的条件下，我们最需要保证，同一时刻只能有一个线程获得锁，这是最基本的一点。

- 防止死锁

  在分布式高并发的条件下，比如有个线程获得锁的同时，还没有来得及去释放锁，就因为系统故障或者其它原因使它无法执行释放锁的命令,导致其它线程都无法获得锁，造成死锁。

- 高性能

  对于访问量大的共享资源，需要考虑减少锁等待的时间，避免导致大量线程阻塞。因此锁的粒度应该尽量小。

- 可重入

  一个线程可以重复拿到同一个资源的锁



## 数据库的唯一索引

获得锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否处于锁定状态。

存在以下几个问题：

- 无法避免死锁，锁没有失效时间，解锁失败的话其它进程无法再获得该锁；
- 只能是非阻塞锁，插入失败直接就报错了，无法重试；
- 不可重入，已经获得锁的进程也必须重新获取锁。



## Redis的setnx命令

使用 `setnx`（Set If Not Exist）指令插入一个键值对，如果 key 已经存在，那么会返回 false ，否则插入成功并返回 true 。

`setnx` 指令和数据库的唯一索引类似，保证了只存在一个 key 的键值对，那么可以用一个 key 的键值对是否存在来判断是否存于锁定状态。

`expire` 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。

```
setnx lock uuid
expire lock 30
```

如果 `setnx` 返回 1，说明该进程获得锁，将 lock 的值设置为锁的超时时间（当前时间 + 锁的有效时间）

如果 `setnx` 返回 0，表示内存中已经有 key，说明其他进程已经获得了锁，进程不能进入临界区，进程可以在一个循环中不断地尝试 `setnx` 操作来获得锁。

但以上方式是不好的，因为这两条命令不是原子操作，如果第一条命令执行后客户端崩溃，那么锁就永远不能被释放造成死锁

在 redis-2.6.12 之后，`set` 命令可以自带 expire 参数

```sh
SET key value [EX seconds|PX milliseconds] [NX|XX] [KEEPTTL]
```

在集群模式下，仅仅使用 `setnx` 仍存在问题：

1. 假设客户端 A 在 Redis 的 master 节点上拿到了锁，然后 master 宕机，存储锁的 key 还没有来得及同步到 slave 上，master 宕机引发故障转移，slave 节点升级为 master 节点，客户端 B 执行加锁操作，它可以从新的 master 获取到锁，这样 A 和 B 就获取到了同一把锁，锁的安全性被打破了。
2. 如果业务代码执行时间大于锁的过期时间，那么锁会被提前释放，别的线程又可以获取到锁。





## Redis的RedLock算法

RedLock 算法使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。

这些 Redis 实例都是独立部署的，彼此之间没有主从关系，避免了 Redis 异步复制造成的锁丢失，例如，主节点刚刚 set 了一条数据，还没来得及同步给从节点就挂了，这时会重新选举出一个新的主节点，此时新主节点中是没有刚刚 set 的那条数据的。

获取锁：

- 尝试从 N 个互相独立 Redis 实例获取锁；
- 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N/2 + 1）实例上获取了锁，才认为获取锁成功；
- 如果获取锁失败，就到每个实例上释放锁。

释放锁：

向所有 Redis 实例发送释放锁命令即可，不需要关心 Redis 实例有没有成功上锁。



## Redisson

Redisson 是在 Redis 基础上实现的一套开源解决方案，其中关于锁的实现源码是基于 `RedissonLock` 类，其中加锁/释放锁操作都是用 LUA 脚本完成的。



1. 获取锁：

   线程去获取锁，获取成功：执行 LUA 脚本，保存数据到 Redis 数据库。

   线程去获取锁，获取失败:一直通过 while 循环尝试获取锁，获取成功后，执行 Lua 脚本，保存数据到 Redis 数据库。

<img src="http://store.secretcamp.cn/uPic/image-20210405112337555202104051123381617593018yodcdayodcda.png" alt="image-20210405112337555" style="zoom: 20%;" />



2. Watch Dog 自动延期机制

   为了避免死锁，一般都会在 Redis 中设置一个过期时间，但是如果业务代码的执行时间大于锁的过期时间，就会启动一个 watch dog 后台线程，不断的延长锁的生存时间。

   这样可以保证锁一定会在业务代码执行完成后才被释放。





## Zookeeper的有序节点

Zookeeper 的有序节点天然适合作为分布式锁：

- 有序节点具有天然的递增有序性，可以确保锁的公平
- 事件监听机制，可以保障占有锁的传递有序而且高效



分布式锁的抢占过程如下：

1. 假设分布式锁的路径为 `/locks` ，每个客户端在 `/locks` 下创建临时有序节点。

   依次创建的有序节点为 `x0000000001` 、`x0000000002` 等

2. 客户端创建临时有序节点，并取得 `/locks` 下的子节点，并进行排序，判断排在最前面的是否为自己
   - 如果自己的锁节点在第一位，代表获取锁成功。
   - 如果自己的锁节点不在第一位，则监听自己前一位的锁节点。
3. 占有锁的客户端执行业务代码后释放锁，删除之前自己创建的临时有序节点。
4. Watcher 通知下一个节点锁已经被释放，下一个节点重复进行步骤 2 的判断。







# 分布式一致性协议🐽

在分布式系统中，每一个节点都可以清楚的知道自己在进行事务操作过程总的结果是成功还是失败，但无法获知其他分布式节点的操作结果。

因此，当一个事务操作需要跨越多个分布式节点时，为了保证事务处理的 ACID 特性，需要引入一个被称为 “协调者” 的组件来统一调度所有分布式节点的执行逻辑。



## 2PC

两阶段提交（Two-phase Commit，2PC），通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。

### 阶段一 投票



### 阶段二 执行



### 2PC的问题



## 3PC





## Paxos算法





# 分布式唯一ID🐽

分布式系统中会对一些数据量大的业务进行分拆，如：用户表，订单表。因为数据量巨大一张表无法承接，就会对其进行分库分表。

一旦涉及到分库分表，就会引申出分布式系统中唯一主键 ID 的生成问题，唯一 ID 的特性：

- 整个系统 ID 唯一
- ID 是数字类型，而且是趋势递增的
- ID 简短，查询效率快



## 基本方案

### UUID

通用唯一识别码（Universally Unique Identifier，UUID）生成的是本机生成的随机的 ID ，不合适。



### MySQL主键自增

利用了 MySQL 的主键自增 *auto_increment* ，默认每次 ID 加 1 。

缺点：

- 存在单点问题，如果 MySQL 挂了，就没法生成 ID 了
- 数据库压力大，高并发抗不住



### MySQL多实例主键自增

这个方案就是解决 MySQL 的单点问题，在 *auto_increment* 在基础上，设置 step 步长

缺点：

- 一旦把步长定好后，就无法扩容；
- 单个数据库的压力大，数据库自身性能无法满足高并发

<img src="http://store.secretcamp.cn/uPic/image-20210811165350805202108111653501628672030u3ltOvu3ltOv.png" alt="image-20210811165350805" style="zoom:40%;" />



### 雪花snowflake算法

<img src="http://store.secretcamp.cn/uPic/image-2021081116564390620210811165644162867220484e5m084e5m0.png" alt="image-20210811165643906" style="zoom:50%;" />

优点：

- 此方案每秒能够产生 409.6 万个ID，性能快
- 时间戳在高位，自增序列在低位，整个 ID 是趋势递增的，按照时间有序递增
- 灵活度高，可以根据业务需求，调整 bit 位的划分，满足不同的需求

缺点：

- 依赖机器的时钟，如果服务器时钟回拨，会导致重复 ID 生成

在分布式场景中，服务器时钟回拨会经常遇到，一般存在 10ms 之间的回拨，一旦回拨，就很有可能存在重复 ID 。



### Redis生成方案

利用 Redis 的 `incr` 原子性操作自增，一般算法为：年份 + 当天距当年第多少天 + 天数 + 小时 + redis自增

优点：

- 有序递增，可读性强

缺点：

- 占用带宽，每次要向 Redis 进行请求



## 改进方案

### 范围获取数据库自增主键

请求数据库得到唯一 ID 的时候，可设计成获得的 ID 是一个 ID 区间段。

在数据库中维护一张 ID 规则表：

1. UserService 在注册一个用户时，需要一个用户ID，会请求 IDGenerateorService 的接口

2. IDGenerateorService 根据 biz_tag 会去查询数据库，查询到现在的 max_id 为 0，step=1000

3. IDGenerateorService 把 max_id 和 step 返回给 UserService，并且把 max_id 更新为max_id = max_id + step，即更新为1000

4. UserService 获得 max_id=0，step=1000

5、 这个用户服务可以用 ID=[max_id + 1，max_id + step] 区间的 ID，即为 [1，1000]

6、UserService 会把这个区间保存到 JVM 中

7、UserService 需要用到 ID 的时候，在区间 [1，1000] 中依次获取 id，可采用 `AtomicLong` 中的 `getAndIncrement()` 方法。

8、如果把区间的值用完了，再去请求 IDGenerateorService 接口

<img src="http://store.secretcamp.cn/uPic/image-20210811171456793202108111714561628673296lGF86RlGF86R.png" alt="image-20210811171456793" style="zoom:50%;" />



### 双本地缓存

对于以上方案，多个 UserService同时获取 ID，同时去请求 IDGenerateorService，在获取 max_id 的时候会存在并发问题。

多个用户服务获取到了各自的ID区间，在高并发场景下，ID 用的很快，如果多个用户服务在某一时刻都用完了，同时去请求 IDGenerateorService ，无论是业务层面上用分布式锁，还是数据库本身的锁，都会造成请求阻塞。

可以利用双 Buffer 来解决，每个服务维护两个本地 Buffer：

1. 当前获取 ID 存储在 buffer1 中，每次获取 ID 在 buffer1 中获取

2. 当 buffer1 中的 Id 已经使用到了区间的 10%，先判断 buffer2 中有没有去获取过，如果没有就立即创建线程发起请求获取 ID ，此线程把获取到的 ID 存储到 buffer2 中。

3. 如果 buffer1 用完了，会自动切换到 buffer2

4. buffer2 用到10%了，也会启动线程再次获取，设置到 buffer1 中

5. 依次往返





# 幂等性🐽

幂等：在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。通俗的说就是一个接口，多次发起同一个请求，必须保证操作只能执行一次。

例如：

- 订单接口，不能多次创建订单

- 支付接口，重复支付同一笔订单只能扣一次钱

- 支付宝回调接口，可能会多次回调，必须处理重复回调

- 普通表单提交接口，因为网络超时等原因多次点击提交，只能成功一次



## 重复请求的产生

- 前端重复提交：提交订单，用户快速重复点击多次，造成后端生成多个内容重复的订单。
- 接口超时重试：对于给第三方调用的接口，为了防止网络抖动或其他原因造成请求丢失，这样的接口一般都会设计成超时重试多次。
- 消息重复消费：MQ 消息中间件，消息重复消费。





## 幂等的实现方式

### 插入前先查询

在保存数据的接口中，我们为了防止产生重复数据，一般会在 `insert` 前，先根据业务数据去查重表中检索一下数据。如果该数据已存在，则执行 `update` 操作或者直接返回，如果不存在，才执行 `insert `操作。



### 上游传递唯一流转id

每次向服务端请求时候附带一个短时间内唯一不重复的序列号，该序列号可以是一个有序 ID，也可以是一个订单号，一般由上游生成，在调用下游服务端接口时作为参数传递。

使用该 “序列号 + 具体业务id” 作为 redis 的 key，在 redis 中查询是否存在对应键值对。



### Token机制

针对前端重复连续多次点击的情况，例如用户购物提交订单，提交订单的接口就可以通过  Token 的机制实现防止重复提交。

主要流程：

1. 服务端提供生成唯一 token 的接口。在分析业务的时候，确定哪些业务存在幂等问题，客户端必须在发起业务请求前，先去获取 token，服务器会把 token 保存到 redis 中。
2. 客户端之后调用业务接口请求，把 token 也作为参数传递过去，一般放在请求头部。
3. 服务器判断 token 是否存在 redis 中，存在表示第一次请求，这时把 redis 中的 token 删除，继续执行业务。
4. 如果判断 token 不存在 redis 中，就表示是重复操作，直接返回，这样就保证了业务代码，不被重复执行。

![image-20210420195549676](http://store.secretcamp.cn/uPic/image-202104201955496762021042019554916189197496WQKJ86WQKJ8.png)













### 状态机

对于很多业务是有一个业务流转状态的，每个状态都有前置状态和后置状态，以及最后的结束状态。

例如流程的 待审批、审批中、驳回、重新发起、审批通过、审批拒绝。订单的待提交、待支付、已支付、取消。

以订单为例，已支付的状态的前置状态只能是待支付，而取消状态的前置状态只能是待支付，通过这种状态机的流转我们就可以控制请求的幂等。

```java
public enum OrderStatusEnum {

    UN_SUBMIT(0, 0, "待提交"),
    UN_PADING(0, 1, "待支付"),
    PAYED(1, 2, "已支付待发货"),
    DELIVERING(2, 3, "已发货"),
    COMPLETE(3, 4, "已完成"),
    CANCEL(0, 5, "已取消");

    // 前置状态
    private int preStatus;

    // 状态值
    private int status;

    // 状态描述
    private String desc;

    OrderStatusEnum(int preStatus, int status, String desc) {
        this.preStatus = preStatus;
        this.status = status;
        this.desc = desc;
    }

    //...
}
```

