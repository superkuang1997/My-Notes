# 从集中式到分布式🌊

## 单体应用的演变

### JEE时期（单体架构）

JEE（Java Platform Enterprise Edition）提供了企业级软件开发的运行环境与开发工具。

JEE 把企业软件划分为 展示层 、业务逻辑层和数据存储层

<img src="http://store.secretcamp.cn/uPic/image-20210907173322559202109071733221631007202IF9s2wIF9s2w.png" alt="image-20210907173322559" style="zoom:40%;" />

最终，各层的组件会被聚合到一起运行在通用的服务器上，如 JBoss、Tomcat 等。



JEE 的分层设计明确了不同团队的分工，职责明确，但缺点也很明显，多数应用都在一个 JVM 中，随着应用的增大，性能会不断下降，业务之间的耦合严重。



### MVC时期（垂直架构）

开源框架开始 成为企业的标配，这个时期比较主流的框架有 Spring、Struts、Hibernate，就是所谓的 SSH 。

Model：模型部分，主要负责具体的业务逻辑和数据存取 

View：视图，主要处理应用的展示部分

Controller：控制器，主要处理用户的交互，它会从视图层读取数据并传递给模型。

MVC 的分层更加简单，框架也更加轻量级，开发效率有了很大的提升，但这一时期的应用最终还是会被打到一个 War 包里，并且部署在 Tomcat 等 Web 服务器里，即使各个企业有自己的规范和约束，但系统的耦 合度一直没有太大的改善。





## 分布式应用的演变

### 早期SOA

单体应用的所有业务都在一个 JVM 中运行，系统耦合严重，水平扩展能力也极其有限，无法满足高并发的要求。

面向服务的架构（SOA）出现了，SOA 将形成独立对外提供服务的组件 ，每个组件通过网络协议对外提供服务。

网络协议可以是 TCP、HTTP 等，正是通过这种协议，服务之间可以通过接口进行交互。

常见的 SOA 实现方式有两种：

- Web Service：使用 SOAP 协议，即用 HTTP 或 HTTPS 来传输 XML 数据。

  缺点：通信协议笨重，服务化管理设置不完善

  <img src="http://store.secretcamp.cn/uPic/image-20210907175307951202109071753081631008388kRpbbqkRpbbq.png" alt="image-20210907175307951" style="zoom:50%;" />

- ESB（企业服务总线）：服务之间的通信与调用都通过总线来完成，因此 ESB 没有注册中心一说

  缺点：ESB 本身就是一个很重的东西 ，系统的变更可能又反过来影响总线的变更 。

  <img src="http://store.secretcamp.cn/uPic/image-20210907175325090202109071753251631008405nG0zY7nG0zY7.png" alt="image-20210907175325090" style="zoom:50%;" />



### 微服务化

为了解决早期 SOA 中存在的各种问题，近几年的服务化架构得到了进一步的演进 ，逐渐形 成了更加细粒度的微服务架构 。

在微服务架构中，一个应用会被拆分一个个独立 、 运行、可维护的子服务，极大地方便了服务的复用，通过不同的服务编排方式 ，快速产生新的业务逻辑。





## 集中式的特点

集中式系统由一台或多台计算机组成中心节点，数据集中存储于这个中心节点中，并且整个系统的所有业务单元都集中部署在这个中心节点上。

- 部署结构简单，无需考虑多个节点的分布式协作问题

- 存在明显的单点问题，虽然稳定性较好，但一旦出现故障，整个系统将处于不可用状态





## 分布式的特点

分布式系统是一个硬件和软件组件分布在不同网络的计算机上，彼此之间仅通过消息传递进行通信和协调的系统。

同一个分布式系统中的计算机在空间部署上是任意的，可以在不同的机房、城市中。

- 分布式系统中没有主从之分，所有的节点都是对等的
- 缺乏全局时钟，在分布式系统中很难定义事件发生的时间先后









## 分布式存在的问题

- 通信异常

  分布式节点之间需要通过网络进行通信，由于网络本身的不可靠性，每次通信都会存在网络不可用的风险。即使网络通信能够正常进行，其延时也会远远大于单机操作。

- 网络分区

  由于网络发生异常，会导致分布式系统中的部分节点之间网络延时不断增大，最终导致系统中只有部分节点能够正常通信，而另一些节点不能，这种现象叫做「网络分区」。网络分区出现时，分布式系统中会出现局部小集群，极端情况下，这些局部小集群会独立完成原本需要整个分布式系统才能完成的功能。

- 三态

  传统单机系统中，程序调用函数后，可以得到明确的回应，即成功或失败；但在分布式系统中，由于网络是不可靠的，除了成功或失败，还可能出现超时现象。

  - 客户端在发往服务端的途中消息丢失
  - 消息被服务端成功接收并进行了处理，但向客户端回包时消息丢失

- 节点故障

  分布式系统中的每个节点每时每刻都可能会发生故障







# 集群🌊

## 从单机到集群

集群（cluster），是指同一种组件的多个实例，形成的逻辑上的整体。

单机处理到达瓶颈的时候，把单机复制几份，这样就构成了一个 “集群” 。 集群中每台服务器就叫做这个集群的一个 “节点” ，所有节点构成了一个集群。每个节点都提供相同的服务，那么这样系统的处理能力就相当于提升了好几倍。

但用户的请求终究要通过某个节点来处理，最好能够让此时此刻负载较小的节点来处理，这样使得每个节点的压力都比较平均。要实现这个功能，就需要在所有节点之前增加一个 “调度者” 的角色，用户的所有请求都先交给它，然后它根据当前所有节点的负载情况，决定将这个请求交给哪个节点处理。这个 “调度者” 就是负载均衡服务器。



## 从集群到分布式

集群结构的好处就是系统扩展非常容易。如果随着系统业务的发展，当前的系统又支撑不住了，那么给这个集群再增加节点就行了。但是，当业务发展到一定程度的时候，会发现无论怎么增加节点，整个集群性能的提升效果并不明显了。这时候，就需要使用分布式（distributed）结构了。

分布式结构就是将一个完整的系统，按照业务功能，拆分成一个个独立的子系统，在分布式结构中，每个子系统就被称为 “服务” 。这些子系统能够独立运行在 web 容器中，它们之间通过 RPC 方式通信。







# 负载均衡🌊

负载均衡（Load Balance），其含义就是指将负载（工作任务）进行平衡、分摊到多个操作单元上进行运行，例如 FTP 服务器、Web服务器、企业核心应用服务器和其它主要任务服务器等，从而协同完成工作任务。

集群，是指同一种组件的多个实例，形成的逻辑上的整体。集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点。

负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上。

负载均衡器可以用来实现高可用以及伸缩性：

- 高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用；
- 伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。

负载均衡器运行过程包含两个部分：

- 根据负载均衡算法得到转发的节点

- 进行转发。



## 负载均衡的类型

### DNS实现负载均衡

DNS 实现负载均衡是最基础简单的方式。一个域名通过 DNS 解析到多个 IP，每个 IP 对应不同的服务器实例，这样就完成了流量的调度，虽然没有使用常规的负载均衡器，但实现了简单的负载均衡功能。

<img src="http://store.secretcamp.cn/uPic/image-20210313210333697202103132103331615640613UQZ2XcUQZ2Xc.png" alt="image-20210313210333697" style="zoom:50%;" />





### 硬件负载均衡

- LVS（Linux 虚拟服务器）：运行在 OS 内核，工作在传输层，可对更高层次的网络协议进行转发，可以直接告诉请求应该走哪个Nginx 

- F5：是专门的硬件，工作在传输层，性能要比 LVS 更高，价格更贵





### 软件负载均衡

软件负载均衡，可以在普通的服务器上运行负载均衡软件，实现负载均衡功能。目前常见的有 `Nginx`、`HAproxy`、`LVS`。其中的区别：

- `Nginx`：七层负载均衡，支持 HTTP、E-mail 协议，同时也支持 4 层负载均衡；
- `HAproxy`：支持七层规则的，性能也很不错。OpenStack 默认使用的负载均衡软件就是 HAproxy；
- `LVS`：运行在内核态，性能是软件负载均衡中最高的，严格来说工作在三层，所以更通用一些，适用各种应用服务。

软件负载均衡的优点：

- 易操作：无论是部署还是维护都相对比较简单；
- 便宜：只需要服务器的成本，软件是免费的；
- 灵活：4 层和 7 层负载均衡可以根据业务特点进行选择，方便进行扩展和定制功能。





## 负载均衡算法

### 轮询

轮询（Round Robin）算法把每个请求轮流发送到每个服务器上。

下图中，一共有6个客户端产生了6个请求，这6个请求按 [1, 2, 3, 4, 5, 6] 的顺序发送。[1, 3, 5] 的请求会被发送到 Server-1，[2, 4, 6] 的请求会被发送到 Server-2。

<img src="http://store.secretcamp.cn/uPic/image-20210314093239132202103140932391615685559dnaBnPdnaBnP.png" alt="image-20210314093239132" style="zoom: 50%;" />

该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载，如下图中的 Server-2 。

<img src="http://store.secretcamp.cn/uPic/image-20210314093414552202103140934141615685654EgxIGuEgxIGu.png" alt="image-20210314093414552" style="zoom:48%;" />



### 加权轮询

加权轮询（Weighted Round Robbin）是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值。

例如下图中，Server-1 被赋予的权值为5，Server-2 被赋予的权值为1，那么 [1, 2, 3, 4, 5] 请求会被发送到 Server-1，[6] 请求会被发送到 Server-2 。

<img src="http://store.secretcamp.cn/uPic/image-2021031409381245620210314093812161568589284fwZB84fwZB.png" alt="image-20210314093812456" style="zoom:50%;" />

### 最少连接

最少连接（least Connections）算法就是将请求发送给当前最少连接数的服务器上。

例如下图中，Server-1 当前连接数最小，那么新到来的请求6 就会被发送到 Server-1 上。

<img src="http://store.secretcamp.cn/uPic/image-20210314093921385202103140939211615685961IuoFJLIuoFJL.png" alt="image-20210314093921385" style="zoom:50%;" />



### 加权最少连接

加权最少连接（Weighted Least Connection）是在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。



### 随机算法

随机（Random）算法把请求随机发送到服务器上。

和轮询算法类似，该算法比较适合服务器性能差不多的场景。





### 源地址哈希法

源地址哈希通过对客户端 IP 计算哈希值之后，再对服务器数量取模得到目标服务器的序号。

可以保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话粘滞（Sticky Session）





### 一致性哈希

#### 哈希取模

假设三台缓存服务器，用于缓存图片，可以使用公式：

```
hash(图片名称) % N
```



但是上述算法有很大的缺陷：当缓存服务器数量发生变化时，会引起缓存雪崩（大量缓存同一时间失效），因为被除数不变的情况下，除数变了，那么余数一定会发生变化。



#### 一致性哈希算法

一致性哈希算法也是使用取模的方法，只是，普通哈希取模法是对服务器的数量进行取模，而一致性哈希算法是对 2^32 取模。

将 [0 ~ 2<sup>32</sup> - 1] 看成是一个环，经过一致性哈希计算后，服务器会位于环的某个位置。

```
hash(服务器IP) % 2^32
```

<img src="http://store.secretcamp.cn/uPic/image-20210901182817946202109011828181630492098zFS0zmzFS0zm.png" alt="image-20210901182817946" style="zoom: 33%;" />



当需要使用缓存服务器缓存图片时，仍然使用图片的名称作为找到图片的 key ，公式为：

```
hash(图片名称) %  2^32
```

具体存储图片的服务器，是从图片的位置开始，沿顺时针方向遇到的第一个服务器。

<img src="http://store.secretcamp.cn/uPic/image-202109011830171962021090118301716304922179eiRoN9eiRoN.png" alt="image-20210901183017196" style="zoom:40%;" />



#### 虚拟节点

服务器在 hash 环上的位置可能会过于接近，例如下图中大多数图片都缓存在 A 服务器上，没有一张图片缓存在 C 服务器上。

<img src="http://store.secretcamp.cn/uPic/image-20210901183245339202109011832451630492365OllqReOllqRe.png" alt="image-20210901183245339" style="zoom: 33%;" />



对于这种情况，可以使用虚拟节点：

虚拟节点是实际的物理服务器在 hash 环上的复制品，一个实际节点可以对应多个虚拟节点，引入虚拟节点的概念后，缓存的分布就会更加均衡。

<img src="http://store.secretcamp.cn/uPic/image-202109011834043142021090118340416304924440PpGUj0PpGUj.png" alt="image-20210901183404314" style="zoom:40%;" />



#### 算法优点

1. 增删缓存服务器时只会导致部分缓存失效，不会导致全局缓存雪崩
2. 虚拟节点保证不会出现数据倾斜





## 转发实现

### HTTP重定向

HTTP 重定向负载均衡服务器使用某种负载均衡算法计算得到服务器的 IP 地址之后，将该地址写入 HTTP 重定向报文中，状态码为 302，客户端收到重定向报文之后，需要重新向服务器发起请求。

缺点：

- 需要两次请求，因此访问延迟比较高；
- HTTP 负载均衡器处理能力有限，会限制集群的规模。

该负载均衡转发的缺点比较明显，实际场景中很少使用它

<img src="http://store.secretcamp.cn/uPic/image-20210314094631203202103140946311615686391dE2zAidE2zAi.png" alt="image-20210314094631203" style="zoom:50%;" />



### DNS域名解析

在DNS解析域名的同时使用负载均衡算法计算服务器 IP 地址。

优点：

- DNS 能够根据地理位置进行域名解析，返回离用户最近的服务器 IP 地址。

缺点：

- 由于DNS具有多级结构，每一级的域名记录都可能被缓存，当下线一台服务器需要修改DNS记录时，需要过很长一段时间才能生效。

大型网站基本使用了 DNS 做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。也就是说，域名解析的结果为内部的负载均衡服务器 IP 地址。





### 反向代理服务器

反向代理服务器位于源服务器前面，用户的请求需要先经过反向代理服务器才能到达源服务器。反向代理可以用来进行缓存、日志记录等，同时也可以用来做为负载均衡服务器。

在这种负载均衡转发方式下，客户端不直接请求源服务器，因此源服务器不需要外部 IP 地址，而反向代理需要配置内部和外部两套 IP 地址。

优点：

- 与其它功能集成在一起，部署简单。

缺点：

- 所有请求和响应都需要经过反向代理服务器，它可能会成为性能瓶颈。



### 网络层

在操作系统内核进程获取网络数据包，根据负载均衡算法计算源服务器的 IP 地址，并修改请求数据包的目的 IP 地址，最后进行转发。

源服务器返回的响应也需要经过负载均衡服务器，通常是让负载均衡服务器同时作为集群的网关服务器来实现。

优点：

- 在内核进程中进行处理，性能比较高。

缺点：

- 和反向代理一样，所有的请求和响应都经过负载均衡服务器，会成为性能瓶颈。



### 链路层

在链路层根据负载均衡算法计算源服务器的 MAC 地址，并修改请求数据包的目的 MAC 地址，并进行转发。

通过配置源服务器的虚拟 IP 地址和负载均衡服务器的 IP 地址一致，从而不需要修改 IP 地址就可以进行转发。也正因为 IP 地址一样，所以源服务器的响应不需要转发回负载均衡服务器，可以直接转发给客户端，避免了负载均衡服务器的成为瓶颈。

这是一种三角传输模式，被称为直接路由。对于提供下载和视频服务的网站来说，直接路由避免了大量的网络传输数据经过负载均衡服务器。

这是目前大型网站使用最广负载均衡转发方式，在 Linux 平台可以使用的负载均衡服务器为 LVS（Linux Virtual Server）。





# 集群下的Session管理🌊

一个用户的Session信息如果存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的Session信息，那么该用户就需要重新进行登录等操作。



## Sticky Session

Sticky Session（会话粘滞）需要配置负载均衡器，使得一个用户的所有请求都路由到同一个服务器，这样就可以把用户的 Session 存放在该服务器中。

缺点：

- 当服务器宕机时，将丢失该服务器上的所有 Session。

<img src="http://store.secretcamp.cn/uPic/image-202103140957419922021031409574216156870629lJkQp9lJkQp.png" alt="image-20210314095741992" style="zoom:50%;" />



## Session Replication

Session Replication（会话复制）在服务器之间进行同步操作，每个服务器都有所有用户的 Session 信息，因此用户可以向任何一个服务器进行请求。

缺点：

- 占用过多内存；
- 同步过程占用网络带宽以及服务器处理器时间。

<img src="http://store.secretcamp.cn/uPic/image-20210314095722885202103140957231615687043xG0ZZnxG0ZZn.png" alt="image-20210314095722885" style="zoom:50%;" />



## Session Server

使用一个单独的服务器存储 Session 数据，可以使用传统的 MySQL ，也使用 Redis 或者 Memcached 这种内存型数据库。

优点：

- 为了使得大型网站具有伸缩性，集群中的应用服务器通常需要保持无状态，那么应用服务器不能存储用户的会话信息。Session Server 将用户的会话信息单独进行存储，从而保证了应用服务器的无状态。





# CAP🌊

## 一致性

一致性（Consistency）：多个数据副本是否能保持一致的特性，在一致性的条件下，系统在执行数据更新操作之后所有节点在同一时间的数据完全一致。

对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。



## 可用性

可用性（Availabiltity）：分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量。

在可用性条件下，要求系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。



## 分区容错性

网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。

分区容错性（Partition tolerance）：分布式系统中在遇到任何网络分区故障的时候，仍然需要保证对外提供满足一致性或可用性的服务，除非是整个网络环境都发生了故障。

一般来说，系统如果不能在时限内达成数据的最终一致性，就意味着发生了分区的情况。



## 三进二特性

分布式系统不可能同时满足一致性、可用性和分区容忍性，最多只能同时满足其中两项。

- CA：单点集群，满足一致性，可用性的系统，放弃了分区容错性往往意味着放弃了系统的可扩展性 

  例如 MySQL 如果发生了网络分区，很显然就无法达成强一致性。 

  > RDBMS

- CP：满足一致性，分区容忍性的系统，放弃了可用性意味着一旦系统遇到了网络分区或其他故障，那么受影响的服务需要等待一定时间去恢复，系统在这个时间段内不可用。

  > Redis、MongoDB、Hbase

- AP：满足可用性，分区容错性的系统，这里放弃了一致性并不是完全放弃一致性，因为如果系统中的数据完全没有一致性，那么这些数据都是毫无意义的。所以放弃一致性指的是放弃数据的强一致性，保留数据的最终一致性。

  > 大多数网站架构的选择

<img src="http://store.secretcamp.cn/uPic/image-20210316211118187202103162111181615900278VT3GeKVT3GeK.png" alt="image-20210316211118187" style="zoom:33%;" />





在分布式系统中，分区容忍性（P）必不可少，这是一个基本要求，因为分布式系统中必然存在多个节点，同时网络又是不稳定的，所以保证分区容错性就是保证分布式系统正常运行的必然要求。

因此，CAP 理论实际上是要在可用性和一致性之间做权衡。可用性和一致性往往是冲突的，很难使它们同时满足。在多个节点之间进行数据同步时：

- 为了保证一致性（CP），不能访问未同步完成的节点，也就失去了部分可用性
- 为了保证可用性（AP），允许读取所有节点的数据，但是数据可能不一致



# BASE🌊

BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。

BASE 理论是对 CAP 中一致性和可用性权衡的结果，它的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。

BASE 理论的思想是通过让系统放松对某一时刻数据一致性的要求，来换取系統整体伸缩性和性能上改观。

许多大型系统往往由于地域分布和极高性能的要求，不可能采用分布式事务来完成某些指标，要想获得这些指标，我们必须采用另外一种方式来完成，这里 BASE 就是解决这个问题的办法。



## 基本可用

指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。

- 响应时间上的损失：在线搜索引擎原本可以在 0.5 秒返回查询结果，由于出现故障，响应时间增加到1~2秒
- 功能上的损失：电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面



## 软状态

指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在时延。



## 最终一致性

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。

ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。

在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。





# 分布式锁🌊

## 分布式锁的概念

在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。

高效分布式锁的必要条件：

- 互斥

  在分布式高并发的条件下，我们最需要保证，同一时刻只能有一个线程获得锁，这是最基本的一点。

- 防止死锁

  在分布式高并发的条件下，比如有个线程获得锁的同时，还没有来得及去释放锁，就因为系统故障或者其它原因使它无法执行释放锁的命令,导致其它线程都无法获得锁，造成死锁。

- 高性能

  对于访问量大的共享资源，需要考虑减少锁等待的时间，避免导致大量线程阻塞。因此锁的粒度应该尽量小。

- 可重入

  一个线程可以重复拿到同一个资源的锁



## Redis的setnx命令

基本思想：使用 `setnx`（Set If Not Exist）指令插入一个键值对，如果 key 已存在，则返回 false ，否则插入成功并返回 true 。

`setnx` 指令和数据库的唯一索引类似，保证了只存在一个 key 的键值对，那么可以用一个 key 的键值对是否存在来判断是否存于锁定状态。

### step1

`expire` 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。

```
setnx lock uuid
expire lock 30
```

如果 `setnx` 返回 1，说明该进程获得锁，将 lock 的值设置为锁的超时时间（当前时间 + 锁的有效时间）

如果 `setnx` 返回 0，表示内存中已经有 key，说明其他进程已经获得了锁，进程不能进入临界区，进程可以在一个循环中不断地尝试 `setnx` 操作来获得锁，或者等待 100ms 重试。

缺点：这两条命令不是原子操作，如果第一条命令执行后客户端崩溃，那么锁就永远不能被释放造成死锁



### step2

在 redis-2.6.12 之后，`set` 命令可以自带 expire 参数

```sh
SET key value [EX seconds|PX milliseconds] [NX|XX] [KEEPTTL]
```



解决了线程异常或服务器宕机造成的死锁，但还有其他问题

场景一：

1. 线程 A 先抢占到了锁，并设置了这个锁 10 秒以后自动开锁
2. 线程 A 的执行时间大于 10 秒，锁被自动打开了
3. 线程 B 获取到锁，并设置了这个锁 10 秒以后自动开锁
4. 过了 5s，线程 A 执行完任务，把锁释放了

从以上场景中总结出两个问题：

1. 线程逻辑执行时间大于加锁时间，锁会被释放
2. 一个线程可以释放另一个线程加的锁





### step3

设置锁的过期时间时，还需要设置锁的唯一编号（value），主动删除锁的时候，需要判断锁的编号是否和设置的一致，如果一致，则认为是自己设置的锁，可以进行主动删除。

但是如果判断锁的编号成功，但删除锁前锁过期了，这段时间内别的线程很可能获取到了锁，然后这里删除的就是别的线程的锁。

可以用 LUA 脚本执行这两个操作，保证原子性。



### 总结

即使有上述几步的优化，使用 `setnx` 作为分布式锁仍存在安全性问题：

- 集群下无法保证锁安全：假设客户端 A 在 Redis 的 master 节点上拿到了锁，然后 master 宕机，存储锁的 key 还没有来得及同步到 slave 上，master 宕机引发故障转移，slave 节点升级为 master 节点，客户端 B 执行加锁操作，它可以从新的 master 获取到锁，这样 A 和 B 就获取到了同一把锁，锁的安全性被打破了。

- 没有锁失效的通知机制：如果业务代码执行时间大于锁的过期时间，那么锁会被提前释放，别的线程又可以获取到锁。





## Redis的RedLock算法

RedLock 算法使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。

这些 Redis 实例都是独立部署的，彼此之间没有主从关系，避免了 Redis 异步复制造成的锁丢失，例如，主节点刚刚 set 了一条数据，还没来得及同步给从节点就挂了，这时会重新选举出一个新的主节点，此时新主节点中是没有刚刚 set 的那条数据的。



假设有 5 个完全独立的 Redis 主服务器：

获取锁：

- 尝试从 5 个互相独立 Redis 实例获取锁；

- 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从至少 3 个实例（总数一半以上）上获取了锁，才认为获取锁成功；

  > 假设获取锁的时间为1s，锁有效期为5s，那么获取锁时间 > 1s 则放弃获取这个锁，从而尝试获取下个锁

- 如果获取锁失败，就到每个实例上释放锁。

释放锁：

向所有 Redis 实例发送释放锁命令即可，不需要关心 Redis 实例有没有成功上锁。



## Redisson

Redisson 是在 Redis 基础上实现的一套开源解决方案，其中关于锁的实现源码是基于 `RedissonLock` 类，其中加锁/释放锁操作都是用 LUA 脚本完成的。



获取锁：

线程去获取锁，获取成功：执行 LUA 脚本，保存数据到 Redis 数据库。

线程去获取锁，获取失败:一直通过 while 循环尝试获取锁，获取成功后，执行 Lua 脚本，保存数据到 Redis 数据库。

<img src="http://store.secretcamp.cn/uPic/image-20210405112337555202104051123381617593018yodcdayodcda.png" alt="image-20210405112337555" style="zoom: 20%;" />



Watch Dog 自动延期机制：

为了避免死锁，一般都会在 Redis 中设置一个过期时间，但是如果业务代码的执行时间大于锁的过期时间，就会启动一个 Watch Dog 后台线程，不断的延长锁的生存时间，例如每隔 10 秒把 key 的超时时间设为 30s ，这样可以保证锁一定会在业务代码执行完成后才被释放。



优点：

- 性能好：基于 Redis 不解释

- 原子性：Redisson 所有指令都通过 lua 脚本执行

- 锁自动延期：由 Watch Dog 机制实现
- 避免死锁：如果机器宕机，Watch Dog 也就没了，此时就不会延长 key 的过期时间



缺点：

不安全：基于 Redisson 的分布式锁是不安全的

- 长时间的 Stop The World 导致锁失效：如果 Full GC 停顿时间太长，此时 Watch Dog 无法运作会导致锁过期，逻辑还没执行完别的客户端又获取到了锁。
- 依赖系统时钟导致的不安全性：如果 A 客户端 在 1、2、3 节点获取到了锁，然后 3 节点时钟发生向前跳跃导致 3 节点上锁过期，然后 B 客户端获取了3、4、5 节点的锁。
- 节点崩溃会导致锁失效：如果 A 客户端 在 1、2、3 节点获取到了锁，然后 3 节点崩溃再被故障转移重启，B 客户端 在 3、4、5 上获取到锁。



## Zookeeper的有序节点

Zookeeper 的有序节点天然适合作为分布式锁：

- 有序节点具有天然的递增有序性，可以确保锁的公平
- 事件监听机制，可以保障占有锁的传递有序而且高效



分布式锁的抢占过程如下：

1. 假设分布式锁的路径为 `/locks` ，每个客户端在 `/locks` 下创建临时有序节点。

   依次创建的有序节点为 `x0000000001` 、`x0000000002` 等

2. 客户端创建临时有序节点，并取得 `/locks` 下的子节点，并进行排序，判断排在最前面的是否为自己
   - 如果自己的锁节点在第一位，代表获取锁成功。
   - 如果自己的锁节点不在第一位，则监听自己前一位的锁节点。
3. 占有锁的客户端执行业务代码后释放锁，删除之前自己创建的临时有序节点。
4. Watcher 通知下一个节点锁已经被释放，下一个节点重复进行步骤 2 的判断。



优点：

- 分布式协调性：Zookeeper 天生设计定位就是分布式协调，强一致性。锁的模型健壮、简单易用、适合做分布式锁。

- 异步的消息通知机制：有等待锁的队列，Watch 机制可以通知队列的下一个节点获取锁。

- 不会死锁：占有锁的客户端如果崩溃，会断开与 Zookeeper 服务端的连接，根据临时节点的特性，与之相关联的节点会被 Zookeeper 自动删除。

缺点:

- 性能较低：添加和删除节点性能相对 Redis 较低，如果有较多的客户端频繁的申请加锁、释放锁，对于 zk 集群的压力会比较大。





# 分布式一致性协议🌊

在分布式系统中，每一个节点都可以清楚的知道自己在进行事务操作过程总的结果是成功还是失败，但无法获知其他分布式节点的操作结果。

因此，当一个事务操作需要跨越多个分布式节点时，为了保证事务处理的 ACID 特性，需要引入一个被称为 “协调者” 的组件来统一调度所有分布式节点的执行逻辑。



## 2PC

两阶段提交（Two-phase Commit，2PC），通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。



### 阶段一 提交事务请求

1. 事务询问

   协调者向所有的参与者发送事务内容，询问是否可以执行事务操作，并开始等待各参与者的响应。

2. 执行事务

   各参与者节点执行事务操作，并将 Undo 和 Redo 信息记入事务日志中。

3. 各参与者向协调者反馈事务询问的响应

   如果参与者成功执行了事务操作，那么就反馈给协调者 Yes 响应，表示事务可以执行；如果参与者没有成功执行事务，那么就反馈给协调者 No 响应，表示事务不可以执行。



### 阶段二 执行

在阶段二中，协调者会根据各参与者的反馈情况来决定最终是否可以进行事务提交操作，正常情况下，包含以下两种可能：

####  事务正常提交

假如协调者从所有的参与者获得的反馈都是 Yes 响应，那么就会执行事务提交。

1. 发送提交请求

   协调者向所有参与者节点发出 Commit 请求。

2. 事务提交

   参与者接收到 Commit 请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的事务资源

3. 反馈事务提交结果。

   参与者在完成事务提交之后，向协调者发送 Ack 消息

 4. 完成事务。

​	协调者接收到所有参与者反馈的 Ack 消息后，完成事务。

<img src="http://store.secretcamp.cn/uPic/image-20210916151714482202109161517141631776634nEcRqbnEcRqb.png" alt="image-20210916151714482" style="zoom:60%;" />



#### 中断事务

假如任何一个参与者向协调者反馈了 No 响应，或者在等待超时之后，协调者尚无法接收到所有参与者的反馈响应，那么就会中断事务。

1. 发送回滚请求

   协调者向所有参与者节点发出 Rollback 请求。

2. 事务回滚。

   参与者接收到 Rollback 请求后，会利用其在阶段一中记录的 Undo 信息来执行事务回滚操作，并在完成回滚之后释放在整个事务执行期间占用的资源。

3. 反馈事务回滚结果。

   参与者在完成事务回滚之后，向协调者发送 Ack 消息。

4. 中断事务。

   协调者接收到所有参与者反馈的 Ack 消息后，完成事务中断。

<img src="http://store.secretcamp.cn/uPic/image-20210916151752790202109161517521631776672ffn5xEffn5xE.png" alt="image-20210916151752790" style="zoom:50%;" />



### 2PC的问题

1. 同步阻塞：2PC 的整个过程是同步阻塞的

2. 单点问题：一旦协调者出现问题，2PC 整个流程将无法执行

3. 数据不一致：如果在第二阶段，协调者向所有参与者发送 Commit，发送到一半协调者挂了，那么就只有部分参与者接收到了 Commit，这会导致数据不一致。

4. 太过保守：2PC 没有设计较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败。



## 3PC





## Paxos算法

Paxis 是一种基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一。



# 分布式唯一ID🌊

分布式系统中会对一些数据量大的业务进行分拆，如：用户表，订单表。因为数据量巨大一张表无法承接，就会对其进行分库分表。

一旦涉及到分库分表，就会引申出分布式系统中唯一主键 ID 的生成问题，唯一 ID 的特性：

- 整个系统 ID 唯一
- ID 是数字类型，而且是趋势递增的
- ID 简短，查询效率快



## 基本方案

### MySQL主键自增

利用了 MySQL 的主键自增 *auto_increment* ，默认每次 ID 加 1 。

缺点：

- 存在单点问题，如果 MySQL 挂了，就没法生成 ID 了
- 数据库压力大，高并发抗不住



### MySQL多实例主键自增

这个方案就是解决 MySQL 的单点问题，在 *auto_increment* 在基础上，设置 step 步长

缺点：

- 一旦把步长定好后，就无法扩容；
- 单个数据库的压力大，数据库自身性能无法满足高并发

<img src="http://store.secretcamp.cn/uPic/image-20210811165350805202108111653501628672030u3ltOvu3ltOv.png" alt="image-20210811165350805" style="zoom:40%;" />



### 雪花snowflake算法

<img src="http://store.secretcamp.cn/uPic/image-2021081116564390620210811165644162867220484e5m084e5m0.png" alt="image-20210811165643906" style="zoom:50%;" />

集群模式下，每个节点 ID 设置的不同，所以在不同机器上雪花算法的结果是唯一的。

优点：

- 此方案每秒能够产生 409.6 万个ID，性能快
- 时间戳在高位，自增序列在低位，整个 ID 是趋势递增的，按照时间有序递增
- 灵活度高，可以根据业务需求，调整 bit 位的划分，满足不同的需求

缺点：

- 依赖机器的时钟，如果服务器时钟回拨，会导致重复 ID 生成

在分布式场景中，服务器时钟回拨会经常遇到，一般存在 10ms 之间的回拨，一旦回拨，就很有可能存在重复 ID 。



### Redis生成方案

利用 Redis 的 `incr` 原子性操作自增，一般算法为：年份 + 当天距当年第多少天 + 天数 + 小时 + redis自增

优点：

- 有序递增，可读性强

缺点：

- 占用带宽，每次要向 Redis 进行请求





## 改进方案

### 范围获取数据库自增主键

请求数据库得到唯一 ID 的时候，可设计成获得的 ID 是一个 ID 区间段。

在数据库中维护一张 ID 规则表：

1. UserService 在注册一个用户时，需要一个用户ID，会请求 IDGenerateorService 的接口

2. IDGenerateorService 根据 biz_tag 会去查询数据库，查询到现在的 max_id 为 0，step=1000

3. IDGenerateorService 把 max_id 和 step 返回给 UserService，并且把 max_id 更新为max_id = max_id + step，即更新为1000

4. UserService 获得 max_id=0，step=1000

5、 这个用户服务可以用 ID=[max_id + 1，max_id + step] 区间的 ID，即为 [1，1000]

6、UserService 会把这个区间保存到 JVM 中

7、UserService 需要用到 ID 的时候，在区间 [1，1000] 中依次获取 id，可采用 `AtomicLong` 中的 `getAndIncrement()` 方法。

8、如果把区间的值用完了，再去请求 IDGenerateorService 接口

<img src="http://store.secretcamp.cn/uPic/image-20210811171456793202108111714561628673296lGF86RlGF86R.png" alt="image-20210811171456793" style="zoom:50%;" />



### 双本地缓存

对于以上方案，多个 UserService 同时获取 ID，同时去请求 IDGenerateorService，在获取 max_id 的时候会存在并发问题。

多个用户服务获取到了各自的 ID 区间，在高并发场景下，ID 用的很快，如果多个用户服务在某一时刻都用完了，同时去请求 IDGenerateorService ，无论是业务层面上用分布式锁，还是数据库本身的锁，都会造成请求阻塞。

可以利用双 Buffer 来解决，每个服务维护两个本地 Buffer：

1. 当前获取 ID 存储在 buffer1 中，每次获取 ID 在 buffer1 中获取

2. 当 buffer1 中的 Id 已经使用到了区间的 10%，先判断 buffer2 中有没有去获取过，如果没有就立即创建线程发起请求获取 ID ，此线程把获取到的 ID 存储到 buffer2 中。

3. 如果 buffer1 用完了，会自动切换到 buffer2

4. buffer2 用到10%了，也会启动线程再次获取，设置到 buffer1 中

5. 依次往返





# 幂等性🌊

幂等：在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。通俗的说就是一个接口，多次发起同一个请求，必须保证操作只能执行一次。

例如：

- 订单接口，不能多次创建订单

- 支付接口，重复支付同一笔订单只能扣一次钱

- 支付宝回调接口，可能会多次回调，必须处理重复回调

- 普通表单提交接口，因为网络超时等原因多次点击提交，只能成功一次



## 重复请求的产生

- 前端重复提交：提交订单，用户快速重复点击多次，造成后端生成多个内容重复的订单。
- 接口超时重试：对于给第三方调用的接口，为了防止网络抖动或其他原因造成请求丢失，这样的接口一般都会设计成超时重试多次。
- 消息重复消费：MQ 消息中间件，消息重复消费。





## 幂等的实现方式

### 插入前先查询

在保存数据的接口中，我们为了防止产生重复数据，一般会在 `insert` 前，先根据业务数据去查重表中检索一下数据。如果该数据已存在，则执行 `update` 操作或者直接返回，如果不存在，才执行 `insert `操作。



### 上游传递唯一流转id

每次向服务端请求时候附带一个短时间内唯一不重复的序列号，该序列号可以是一个有序 ID，也可以是一个订单号，一般由上游生成，在调用下游服务端接口时作为参数传递。

使用该 “序列号 + 具体业务id” 作为 redis 的 key，在 redis 中查询是否存在对应键值对。



### Token机制

针对前端重复连续多次点击的情况，例如用户购物提交订单，提交订单的接口就可以通过  Token 的机制实现防止重复提交。

主要流程：

1. 服务端提供生成唯一 token 的接口。在分析业务的时候，确定哪些业务存在幂等问题，客户端必须在发起业务请求前，先去获取 token，服务器会把 token 保存到 redis 中。
2. 客户端之后调用业务接口请求，把 token 也作为参数传递过去，一般放在请求头部。
3. 服务器判断 token 是否存在 redis 中，存在表示第一次请求，这时把 redis 中的 token 删除，继续执行业务。
4. 如果判断 token 不存在 redis 中，就表示是重复操作，直接返回，这样就保证了业务代码，不被重复执行。

![image-20210420195549676](http://store.secretcamp.cn/uPic/image-202104201955496762021042019554916189197496WQKJ86WQKJ8.png)













### 状态机

对于很多业务是有一个业务流转状态的，每个状态都有前置状态和后置状态，以及最后的结束状态。

例如流程的 待审批、审批中、驳回、重新发起、审批通过、审批拒绝。订单的待提交、待支付、已支付、取消。

以订单为例，已支付的状态的前置状态只能是待支付，而取消状态的前置状态只能是待支付，通过这种状态机的流转我们就可以控制请求的幂等。

```java
public enum OrderStatusEnum {

    UN_SUBMIT(0, 0, "待提交"),
    UN_PADING(0, 1, "待支付"),
    PAYED(1, 2, "已支付待发货"),
    DELIVERING(2, 3, "已发货"),
    COMPLETE(3, 4, "已完成"),
    CANCEL(0, 5, "已取消");

    // 前置状态
    private int preStatus;

    // 状态值
    private int status;

    // 状态描述
    private String desc;

    OrderStatusEnum(int preStatus, int status, String desc) {
        this.preStatus = preStatus;
        this.status = status;
        this.desc = desc;
    }

    //...
}
```





# 网关🌊

微服务下一个系统被拆分为多个服务，但是像安全认证、流量控制、日志、监控等功能是每个服务都需要的，没有网关的话，就需要在每个服务中单独实现，这会导致做了很多重复的事情并且没有一个全局的视图来统一管理这些功能。







# 限流🌊

限流（Rate Limit）：只允许指定的事件进入系统，超过的部分将被拒绝服务、排队或等待、降级等处理。



限流与熔断容易被混淆，限流主要在 Server 实现，而熔断主要在 Client 实现，但是一个服务既可以充当 Server 也可以充当 Client，这也是让限流与熔断可以同时存在一个服务中。



## 限流算法

### 固定窗口计数器算法

规定单位时间处理的请求数量。

比如我们规定我们的一个接口一分钟只能访问 10 次，使用固定窗口计数器算法的话可以这样实现：

给定一个变量 counter 来记录处理的请求数量，当 1 分钟之内处理一个请求之后 counter++ ， 1 分钟之内的如果 counter=100 的话，后续的请求就会被全部拒绝。等到 1分钟结束后，将 counter 回归成0，重新开始计数。

这种限流算法无法保证限流速率，因而无法保证突然激增的流量。比如我们限制一个接口一分钟只能访问 10 次的话，前半分钟一个请求没有接收，后半分钟接收了 10 个请求。





### 滑动窗口计数器法

滑动窗口计数器把时间以一定比例分片。

例如我们的接口限流每分钟处理 60 个请求，可以把 1 分钟分为 60 个窗口。每隔 1 秒移动一次，每个窗口一秒只能处理不大于 60 的请求， 如果当前窗口的请求计数总和超过了限制的数量的话就不再处理其他请求。

当滑动窗口的格子划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确。





### 漏桶算法

可以把发请求的动作比作成注水到桶中，我们处理请求的过程可以比喻为漏桶漏水。我们往桶中以任意速率流入水，以一定速率流出水。当水超过桶流量则丢弃，因为桶容量是不变的，保证了整体的速率。如果想要实现这个算法的话也很简单，准备一个队列用来保存请求，然后我们定期从队列中拿请求来执行就好了。

<img src="http://store.secretcamp.cn/uPic/image-20210825163811430202108251638111629880691I2eVQoI2eVQo.png" alt="image-20210825163811430" style="zoom:40%;" />



### 令牌桶算法

和漏桶算法算法相似，不过现在桶里装的是令牌了，请求在被处理之前需要拿到一个令牌，请求处理完毕之后将这个令牌丢弃，根据限流大小，按照一定的速率往桶里添加令牌。

<img src="http://store.secretcamp.cn/uPic/image-202108251639386102021082516393816298807786RXo2c6RXo2c.png" alt="image-20210825163938610" style="zoom:40%;" />



### 四种算法的对比

- 固定窗口：实现简单，但是过于粗暴，除非情况紧急，为了能快速止损眼前的问题可以作为临时应急的方案。
- 滑动窗口：限流算法简单易实现，可以应对有少量突增流量场景。
- 漏桶：对于流量绝对均匀有很强的要求，资源的利用率上不是极致，但其宽进严出模式，保护系统的同时还留有部分余量，是一个通用性方案。
- 令牌桶：系统经常有突增流量，并尽可能的压榨服务的性能。



## 限流的分类

根据限流的粒度分类：

- 单机限流
- 分布式限流



### 单机限流

单机限流是指请求进入到某一个服务节点后超过了限流阈值，服务节点采取了一种限流保护措施。

现状的系统基本上都是分布式架构，单机的模式已经很少了。

<img src="http://store.secretcamp.cn/uPic/image-202108262046289432021082620462916299819893dWsXB3dWsXB.png" alt="image-20210826204628943" style="zoom:40%;" />



### 分布式限流

单机限流防止流量压垮服务节点，但缺乏对整体流量的感知。分布式限流则适合做细粒度不同的限流控制，可以根据场景不同匹配不同的限流规则。

与单机限流最大的区别，分布式限流需要中心化存储，常见的使用 Redis 实现。





## 限流的应用

### Guava RateLimiter



### hystrix





# 服务降级🌊

服务降级是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行，主要的目的就是维持服务，尽管服务有损但是总比没有强。









# 熔断🌊

熔断：调用方访问服务时通过断路器作为代理进行访问，断路器会持续观察服务返回的状态，当失败超过设置的阈值时断路器打开，请求就不能再访问到服务。

降级的目的在于应对系统自身的故障，而熔断的目的在于应对当前系统依赖的外部系统或者第三方系统的故障。





